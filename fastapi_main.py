from fastapi import FastAPI, Request, UploadFile, File, Form, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, RedirectResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
import os, subprocess, csv, json
from pathlib import Path
from typing import List, Dict

# ADD sessions
import uuid, subprocess, json, re

app = FastAPI()

UPLOAD_DIR = Path("src/uploads")
OUTPUT_DIR = Path("src/outputs")    # ìƒì„±ë¬¼ì´ ì €ì¥ë˜ëŠ” í´ë”

UPLOAD_DIR.mkdir(exist_ok=True)
OUTPUT_DIR.mkdir(exist_ok=True)

# === Add: path â†’ /outputs URL ë§¤í•‘ ìœ í‹¸ ===
# [ADD] ë‹¨ê³„(íˆ´)ë³„ ìƒíƒœÂ·ì•„í‹°íŒ©íŠ¸ ì •ë¦¬ ìœ í‹¸
import re

# ADD
# íŒŒì¼ ìƒë‹¨ import ì•„ë˜ì— ì¶”ê°€
PROJECT_ROOT = Path(__file__).resolve().parent
NPX = "npx.cmd" if os.name == "nt" else "npx"

def run_ts_workflow(file_path: Path, sessionId:str, message: str = "ë¶„ì„í•´ì¤˜"):
    import shlex, subprocess, os
    # â¬‡ï¸ ì—¬ê¸°: main.ts + --mode=workflow
    base_args = [NPX, "ts-node", "src/main.ts", "--mode=workflow", message, str(file_path), sessionId]
    env = os.environ.copy()
    try:
        proc = subprocess.Popen(
            base_args,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            shell=False,
            text=True,
            encoding="utf-8",
            errors="replace",
            cwd=str(PROJECT_ROOT),
            env=env,
        )
        stdout, stderr = proc.communicate(timeout=600)
        return proc.returncode, stdout, stderr
    except FileNotFoundError:
        cmd_str = f'{NPX} ts-node src/main.ts --mode=workflow {shlex.quote(message)} {shlex.quote(str(file_path))}'
        proc = subprocess.Popen(
            cmd_str,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            shell=True,
            text=True,
            encoding="utf-8",
            errors="replace",
            cwd=str(PROJECT_ROOT),
            env=env,
        )
        stdout, stderr = proc.communicate(timeout=600)
        return proc.returncode, stdout, stderr


def extract_json_and_text(output_str: str):
    """
    LLM ì¶œë ¥ì—ì„œ JSONê³¼ ìì—°ì–´ ì„¤ëª…ì„ ë¶„ë¦¬.
    ì˜ˆ: "ì„¤ëª…ë¬¸ ... {json...}" â†’ ("ì„¤ëª…ë¬¸", {...})
    """
    json_part = None
    text_part = output_str
    m = re.search(r"(\{[\s\S]*\})", output_str)
    if m:
        try:
            json_part = json.loads(m.group(1))
            text_part = output_str[:m.start()] + output_str[m.end():]
        except Exception:
            pass
    return text_part.strip(), json_part


def format_tool_output(parsed_json, sessionId):
    """LLM ë˜ëŠ” Tool JSONì„ Markdown í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ (Toolë³„ ì„¹ì…˜ ìŠ¤íƒ€ì¼)"""
    emoji_map = {
        "BasicAnalysisTool": "ğŸ§®",
        "CorrelationTool": "ğŸ“Š",
        "SelectorTool": "ğŸ§©",
        "PreprocessingTool": "ğŸ§¼",
        "VisualizationTool": "ğŸ“ˆ",
        "MachineLearningTool": "ğŸ¤–",
        "WorkflowTool": "âš™ï¸",
    }

    md = ""
    if "answers" in parsed_json:
        for ans in parsed_json["answers"]:
            tool = ans.get("tool", "")
            summary = ans.get("summary", "")
            args = ans.get("arguments", {})
            icon = emoji_map.get(tool, "ğŸ”¹")

            md += f"### {icon} {tool}\n"
            if summary:
                md += f"- **ìš”ì•½:** {summary}\n\n"

            if "columnStats" in args:
                md += "| ì»¬ëŸ¼ | íƒ€ì… | ê²°ì¸¡ì¹˜ | ê³ ìœ ê°’ |\n|------|------|--------|--------|\n"
                for c in args["columnStats"]:
                    md += f"| {c.get('column','-')} | {c.get('dtype','-')} | {c.get('missing',0)} | {c.get('unique','-')} |\n"
                md += "\n"

            if "selectedColumns" in args:
                cols = ", ".join(args["selectedColumns"]) or "â€”"
                md += f"**ì„ íƒëœ ì»¬ëŸ¼:** {cols}\n\n"

            if "recommendedPairs" in args:
                pairs = args["recommendedPairs"]
                if pairs:
                    md += "**ì¶”ì²œ í˜ì–´:**\n"
                    for p in pairs:
                        md += f"- {p['column1']} Ã— {p['column2']}\n"
                    md += "\n"

            if "mlModelRecommendation" in args:
                rec = args["mlModelRecommendation"]
                if rec:
                    md += f"**ì¶”ì²œ ëª¨ë¸:** {rec.get('model','â€”')}  \n"
                    if rec.get("score") is not None:
                        md += f"**ì •í™•ë„:** {rec['score']*100:.1f}%  \n"
                    if rec.get("reason"):
                        md += f"ğŸ§  {rec['reason']}\n\n"

            if "comment" in args or "description" in args:
                desc = args.get("comment") or args.get("description")
                md += f"ğŸ§  **ì„¤ëª…:** {desc}\n\n"

            md += "---\n\n"
        return md

    elif "arguments" in parsed_json:
        args = parsed_json["arguments"]
        md += "### ğŸ§© Tool ê²°ê³¼\n"
        if "columnStats" in args:
            md += "| ì»¬ëŸ¼ | íƒ€ì… | ê²°ì¸¡ì¹˜ | ê³ ìœ ê°’ |\n|------|------|--------|--------|\n"
            for c in args["columnStats"]:
                md += f"| {c.get('column','-')} | {c.get('dtype','-')} | {c.get('missing',0)} | {c.get('unique','-')} |\n"
            md += "\n"
        if "summary" in args:
            md += f"ğŸ§  **ì„¤ëª…:** {args['summary']}\n\n"
        return md
    else:
        return f"```json\n{json.dumps(parsed_json, indent=2, ensure_ascii=False)}\n```"


def _norm(p: str) -> str:
    return str(p).replace("\\", "/")

def path_to_outputs_url(path: str | None,sessionId : str) -> str | None:
    if not path:
        return None
    p = Path(path)
    try:
        rel = p.relative_to(OUTPUT_DIR)
        return f"/outputs/{sessionId}/{_norm(rel)}"
    except Exception:
        return f"/outputs/{sessionId}/{_norm(p.name)}"

def map_artifacts(workflow: dict, sessionId: str) -> dict:
    if not isinstance(workflow, dict):
        return workflow
    wf = dict(workflow)

    # 1) ì „ì²˜ë¦¬ ì‚°ì¶œë¬¼ URL
    if wf.get("preprocessedFilePath"):
        wf["preprocessedFilePathUrl"] = path_to_outputs_url(wf["preprocessedFilePath"], sessionId)

    # 2) ML ê²°ê³¼ í‘œì¤€í™”
    #    mlResultPathê°€ ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  ì¼€ì´ìŠ¤(dict/str/None/ê¸°íƒ€)ë¥¼ ì•ˆì „ ì²˜ë¦¬
    raw = wf.get("mlResultPath")
    mlp = {}

    if isinstance(raw, dict):
        mlp = raw
    elif isinstance(raw, str):
        # ë¬¸ìì—´ë§Œ ì˜¤ë©´ ë³´ê³ ì„œ ê²½ë¡œë¡œ ê°„ì£¼
        mlp = {"reportPath": raw}
    elif raw is None:
        mlp = {}
    else:
        # ë¦¬ìŠ¤íŠ¸/íŠœí”Œ/ê¸°íƒ€ê°€ ì˜¬ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ë°©ì–´
        try:
            mlp = dict(raw)  # (key, value) ì‹œí€€ìŠ¤ë©´ ë³€í™˜ë  ìˆ˜ ìˆìŒ
        except Exception:
            mlp = {}

    # reportPath / mlResultPath ë“± ê²½ë¡œ í‚¤ì—ì„œ URL íŒŒìƒ (ë‘˜ ë‹¤ ì§€ì›)
    if isinstance(mlp.get("mlResultPath"), str):
        mlp["mlResultUrl"] = path_to_outputs_url(mlp["mlResultPath"], sessionId)
    if isinstance(mlp.get("reportPath"), str):
        mlp["reportUrl"] = path_to_outputs_url(mlp["reportPath"], sessionId)
        # ë³´ê³ ì„œ ë³¸ë¬¸ì´ 'ï¿½' ë“± ê¹¨ì ¸ìˆë‹¤ë©´ UTF-8 ì¬ë¡œë“œ ì‹œë„
        if mlp.get("report") and "ï¿½" in mlp["report"]:
            try:
                mlp["report"] = Path(mlp["reportPath"]).read_text(encoding="utf-8", errors="ignore")
            except Exception:
                pass

    wf["mlResultPath"] = mlp  # í‘œì¤€í™”ëœ í˜•íƒœë¡œ ë˜ëŒë ¤ ë„£ê¸°

    if isinstance(wf.get("chartPaths"), list):
        wf["chartUrls"] = [path_to_outputs_url(p, sessionId) for p in wf["chartPaths"]]
    return wf

def build_steps(wf: dict, corr_has_table: bool = False) -> list[dict]:  # [CHANGED]
    """ì›Œí¬í”Œë¡œ dictì—ì„œ ë‹¨ê³„(íˆ´)ë³„ ì™„ë£Œ ì—¬ë¶€ë¥¼ ê³„ì‚°"""
    def st(key, title, ok):
        return {"key": key, "title": title, "status": "done" if ok else "skipped"}

    steps = []
    steps.append(st("basic",     "1) BasicAnalysisTool",            bool(wf.get("columnStats"))))
    steps.append(st("corr",      "2) Correlation",                bool(corr_has_table)))  # [NEW]
    steps.append(st("selector",  "3) SelectorTool",                 bool(wf.get("selectedColumns") or wf.get("recommendedPairs") or wf.get("preprocessingRecommendations"))))
    steps.append(st("visual",    "4) VisualizationTool",            bool(wf.get("chartUrls"))))
    steps.append(st("preprocess","5) PreprocessExecutorTool",       bool(wf.get("preprocessedFilePathUrl"))))
    ml_ok = bool( (wf.get("mlModelRecommendation") and wf["mlModelRecommendation"].get("model")) or
                  (wf.get("mlResultPath") and (wf["mlResultPath"].get("mlResultUrl") or wf["mlResultPath"].get("reportUrl"))) )
    steps.append(st("train",     "6) MachineLearningTool",          ml_ok))
    return steps

def coerce_to_json(s: str):
    """
    ë¡œê·¸ì— ì—¬ëŸ¬ JSON-ìœ ì‚¬ ë¸”ë¡ì´ ì„ì—¬ ìˆì„ ë•Œ,
    - í‚¤ì›Œë“œ í¬í•¨ ë¸”ë¡(columnStats ë“±) ìš°ì„ 
    - ì—†ìœ¼ë©´ ê°€ì¥ í° ë¸”ë¡
    ì„ ê³¨ë¼ ë³´ì • í›„ json.loads ì‹œë„.
    """
    # 0) ì •ìƒ JSON ë¨¼ì €
    try:
        return json.loads(s)
    except Exception:
        pass

    if not s or "{" not in s or "}" not in s:
        return None

    # 1) ëª¨ë“  ìµœìƒìœ„ {â€¦} ë¸”ë¡ ì¶”ì¶œ (ë¬¸ìì—´/ì´ìŠ¤ì¼€ì´í”„ ì¸ì§€)
    blocks = []
    depth = 0
    in_str = False
    esc = False
    start = None
    for i, ch in enumerate(s):
        if in_str:
            if esc:
                esc = False
            elif ch == "\\":
                esc = True
            elif ch == '"':
                in_str = False
            continue
        if ch == '"':
            in_str = True
            continue
        if ch == "{":
            if depth == 0:
                start = i
            depth += 1
        elif ch == "}":
            depth -= 1
            if depth == 0 and start is not None:
                blocks.append(s[start:i+1])
                start = None

    if not blocks:
        return None

    # 2) í‚¤ì›Œë“œê°€ ë“¤ì–´ìˆëŠ” ë¸”ë¡ì„ ìš°ì„ , ì—†ìœ¼ë©´ ê¸¸ì´ìˆœ ë‚´ë¦¼ì°¨ìˆœ
    prefer = ("columnStats", "selectedColumns", "mlModelRecommendation", "mlResultPath")
    blocks.sort(key=lambda b: (any(k in b for k in prefer), len(b)), reverse=True)

    # 3) ê° ë¸”ë¡ì— ëŒ€í•´ ë³´ì • í›„ íŒŒì‹± ì‹œë„
    for core in blocks:
        try:
            # { key: ... } -> { "key": ... }
            core2 = re.sub(r'([,{]\s*)([A-Za-z_][A-Za-z0-9_]*)\s*:', r'\1"\2":', core)
            # ' -> "
            core2 = core2.replace("'", '"')
            # JS íŠ¹ìˆ˜ê°’ â†’ JSON ê°’
            core2 = core2.replace("undefined", "null")
            core2 = re.sub(r'\bNaN\b', 'null', core2)
            core2 = re.sub(r'\bInfinity\b', 'null', core2)
            core2 = re.sub(r'\b-Infinity\b', 'null', core2)
            # [Object] â†’ {}
            core2 = re.sub(r'\[\s*Object\s*\]', "{}", core2)
            core2 = core2.replace("[Object], [Object]", "{}, {}")
            # ë ì½¤ë§ˆ ì œê±°
            core2 = re.sub(r',\s*([}\]])', r'\1', core2)
            return json.loads(core2)
        except Exception:
            continue

    return None

import re, time, json
ANSI_RE = re.compile(r"\x1b\[[0-9;]*m")

def sanitize_stdout(s: str) -> str:
    if not s: return ""
    return ANSI_RE.sub("", s).strip()

def _looks_like_workflow(obj: dict) -> bool:
    """ì›Œí¬í”Œë¡œ í•µì‹¬ í‚¤ê°€ 1ê°œë¼ë„ ìˆì–´ì•¼ ìœ íš¨ë¡œ ê°„ì£¼"""
    if not isinstance(obj, dict):
        return False
    keys = {
        "columnStats", "selectedColumns", "recommendedPairs",
        "preprocessingRecommendations", "preprocessedFilePath",
        "preprocessedFilePathUrl", "mlModelRecommendation",
        "mlResultPath", "chartPaths", "chartUrls"
    }
    return any(k in obj for k in keys)

def _jsonify_js_like(text: str) -> str:
    """JSí’ ê°ì²´/ë°°ì—´ ë¬¸ìì—´ì„ JSONìœ¼ë¡œ ê·¼ì‚¬ ë³€í™˜"""
    s = text
    # í‚¤ì— ìŒë”°ì˜´í‘œ ì—†ìœ¼ë©´ ì¶”ê°€: { key: ... } => { "key": ... }
    s = re.sub(r'([{\[,]\s*)([A-Za-z_][A-Za-z0-9_]*)\s*:', r'\1"\2":', s)
    # ' -> "
    s = s.replace("'", '"')
    # NaN/Infinity ê³„ì—´
    s = re.sub(r'\bNaN\b', 'null', s)
    s = re.sub(r'\b-Infinity\b', 'null', s)
    s = re.sub(r'\bInfinity\b', 'null', s)
    # ë ì½¤ë§ˆ ì œê±°
    s = re.sub(r',\s*([}\]])', r'\1', s)
    return s

def extract_workflow_dict(output_str: str):
    """
    1) coerce_to_json
    2) ```json ... ``` ì½”ë“œë¸”ë¡
    3) ìµœìƒìœ„ JSONì˜ answers[*].message.content ë‚´ë¶€ JSON
    4) ë¡œê·¸ í…ìŠ¤íŠ¸ì˜ 'BasicAnalysisTool ê²°ê³¼: [ ... ]' íŒ¨í„´ ì¬êµ¬ì„± â†’ {'columnStats': [...]}
    ì‹¤íŒ¨ ì‹œ (None, None)
    """
    s = sanitize_stdout(output_str)

    m = re.search(r"<<<WORKFLOW_JSON_START>>>\s*([\s\S]*?)\s*<<<WORKFLOW_JSON_END>>>", s)
    if m:
        try:
            obj = json.loads(m.group(1))
            cand = obj.get("workflow") if isinstance(obj, dict) else None
            if isinstance(cand, dict):
                return cand, obj
        except Exception:
            pass


    # 1) 1ì°¨: ê¸°ì¡´ ë³´ì • íŒŒì„œ
    top = coerce_to_json(s)
    if isinstance(top, dict):
        for cand in (top.get("workflow"), top.get("result"), top):
            if isinstance(cand, dict) and _looks_like_workflow(cand):
                return cand, top

    # 2) ```json ... ``` ì½”ë“œë¸”ë¡
    for m in re.finditer(r"```(?:json)?\s*([\s\S]*?)```", s, re.I):
        block = m.group(1).strip()
        try:
            obj = json.loads(block)
            for cand in (obj.get("workflow"), obj.get("result"), obj):
                if isinstance(cand, dict) and _looks_like_workflow(cand):
                    return cand, obj
        except Exception:
            pass

    # 3) answers[*].message.content ë‚´ë¶€ JSON
    try:
        maybe = json.loads(s)
        if isinstance(maybe, dict):
            for a in (maybe.get("answers") or []):
                content = ((a.get("message") or {}).get("content") or "").strip()
                if not content:
                    continue
                try:
                    obj = json.loads(content)
                except Exception:
                    obj = coerce_to_json(content.replace('\\"','"'))
                if isinstance(obj, dict):
                    for cand in (obj.get("workflow"), obj.get("result"), obj):
                        if isinstance(cand, dict) and _looks_like_workflow(cand):
                            return cand, obj
    except Exception:
        pass

    # 4) ë¡œê·¸ í…ìŠ¤íŠ¸ì—ì„œ BasicAnalysisTool ë°°ì—´ë§Œì´ë¼ë„ ì¶”ì¶œ
    m = re.search(r"BasicAnalysisTool\s*ê²°ê³¼\s*:\s*(\[[\s\S]*?\])", s, re.I)
    if m:
        arr_text = _jsonify_js_like(m.group(1))
        try:
            arr = json.loads(arr_text)
            if isinstance(arr, list) and arr and isinstance(arr[0], dict):
                wf = {"columnStats": arr}
                return wf, {"columnStats": arr}
        except Exception:
            pass

    return None, None


# ------------------------------
# ìƒì„±ë¬¼ ë¦¬ìŠ¤íŠ¸
# ------------------------------
def list_generated_files(sessionId:str) -> List[Dict]:
    """OUTPUT_DIR ë‚´ ìƒì„±ë¬¼ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ë¥¼ dict ëª©ë¡ìœ¼ë¡œ ë°˜í™˜"""
    files = []
    if not sessionId:
        # ì ì ˆí•œ ê¸°ë³¸ê°’ ë˜ëŠ” ì—ëŸ¬ ì²˜ë¦¬
        sessionId = "default"  # ì„ì‹œ ê¸°ë³¸ ë””ë ‰í† ë¦¬
    session_dir = OUTPUT_DIR / sessionId
    if OUTPUT_DIR.exists():
        for p in sorted(session_dir.glob("*")):
            if p.is_file():
                files.append({
                    "name": p.name,
                    "url": f"/outputs/{sessionId}/{p.name}",
                    "size": p.stat().st_size,
                    "ext": p.suffix.lower(),
                })
    return files


# Templates ì„¤ì •
templates = Jinja2Templates(directory="templates")

# CORS ì„¤ì • (í•„ìš” ì—†ìœ¼ë©´ ì‚­ì œí•´ë„ ë¨)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ------------------------------
# CSV ë¯¸ë¦¬ë³´ê¸°
# ------------------------------
def get_csv_preview(file_path: str):
    head_columns, head_rows = [], []
    describe_columns, describe_rows = [], []

    try:
        import pandas as pd
        df = pd.read_csv(file_path)
        head_rows = df.head().to_dict(orient="records")
        head_columns = df.columns.tolist()

        describe_df = df.describe(include="all").reset_index()
        describe_rows = describe_df.to_dict(orient="records")
        describe_columns = describe_df.columns.tolist()
    except Exception as e:
        print(f"CSV ë¯¸ë¦¬ë³´ê¸° ì˜¤ë¥˜: {e}")

    return head_columns, head_rows, describe_columns, describe_rows


# ìƒì„±ë¬¼ í´ë”ë¥¼ /outputs ê²½ë¡œë¡œ ì •ì  ì„œë¹™
app.mount("/outputs", StaticFiles(directory=str(OUTPUT_DIR)), name="outputs")

# ------------------------------
# ì„¸ì…˜ ê´€ë¦¬
# ------------------------------
session_files: Dict[str, str] = {}
chat_histories: Dict[str, List[Dict]] = {}

# ------------------------------
# í™ˆ
# ------------------------------
@app.get("/", response_class=HTMLResponse)
async def home(request: Request, sessionId: str = Query(None)):
    # ADD
    file_path = session_files.get(sessionId)
    chat_history = chat_histories.get(sessionId, [])
    
    head_columns = []
    head_rows = []
    describe_columns = []
    describe_rows = []

    # if filename:
    #     file_path = UPLOAD_DIR / filename
    #     try:
    #         with file_path.open(newline="", encoding="utf-8") as csvfile:
    #             reader = csv.DictReader(csvfile)
    #             head_columns = reader.fieldnames or []
    #             for i, row in enumerate(reader):
    #                 if i >= 5:
    #                     break
    #                 head_rows.append(row)
    #     except Exception as e:
    #         print(f"CSV ì½ê¸° ì˜¤ë¥˜: {e}")
    # if filename:
    #     file_path = UPLOAD_DIR / filename
    #     if file_path.exists():
    #         # âœ… pandas ê¸°ë°˜ ë¯¸ë¦¬ë³´ê¸° + ê¸°ìˆ í†µê³„
    #         head_columns, head_rows, describe_columns, describe_rows = get_csv_preview(str(file_path))
    if file_path:
        head_columns, head_rows, describe_columns, describe_rows = get_csv_preview(file_path)

    generated_files = list_generated_files(sessionId)

    # ì´ë¯¸ì§€ ë¯¸ë¦¬ë³´ê¸°ìš© ìƒì„±ë¬¼ (í™•ì¥ì ê¸°ì¤€)
    preview_images = [f for f in generated_files if f["ext"] in {".png", ".jpg", ".jpeg", ".gif", ".webp"}]

    return templates.TemplateResponse("index.html", {
        "request": request,
        "head_columns": head_columns,
        "head_rows": head_rows,
        "describe_columns": describe_columns,
        "describe_rows": describe_rows,
        "current_filename": Path(file_path).name if file_path else None,
        "current_session": sessionId,
        "generated_files": generated_files,
        "preview_images": preview_images,
        "corr": {"headers": [], "rows": []},  # [NEW]
    })


# ------------------------------
# CSV ì—…ë¡œë“œ
# ------------------------------
@app.post("/upload_csv/")
async def upload_csv(request: Request, file: UploadFile = File(...)):
    sessionId = str(uuid.uuid4())
    session_dir = UPLOAD_DIR / sessionId
    session_dir.mkdir(exist_ok=True, parents=True)

    file_path = session_dir / file.filename

    with file_path.open("wb") as f:
        f.write(await file.read())

    # sessionId ê¸°ì¤€ìœ¼ë¡œ ì €ì¥
    session_files[sessionId] = str(file_path)
    chat_histories[sessionId] = []

    head_columns, head_rows, describe_columns, describe_rows = get_csv_preview(str(file_path))

    return templates.TemplateResponse("index.html", {
        "request": request,
        "current_filename": file.filename,
        "current_session": sessionId,
        "chat_history": chat_histories[sessionId],
        "workflow": None,
        "steps": [],
        "generated_files": list_generated_files(sessionId),
        "preview_images": [],
        "head_columns": head_columns,
        "head_rows": head_rows,
        "describe_columns": describe_columns,
        "describe_rows": describe_rows,
        "corr": {"headers": [], "rows": []},  # [NEW]
    })

# [NEW] ì—…ë¡œë“œ í´ë” ë‚´ Correlation CSV ê²½ë¡œ ì¶”ì • + ë¡œë”© ------------------------
def _load_corr_csv(csv_path: str | None) -> dict:  # [NEW]
    if not csv_path:
        return {"headers": [], "rows": []}
    try:
        with open(csv_path, newline="", encoding="utf-8") as f:
            reader = csv.reader(f)
            rows = list(reader)
        if not rows:
            return {"headers": [], "rows": []}
        headers = rows[0][1:]
        body = [{"row": r[0], "vals": r[1:]} for r in rows[1:]]
        return {"headers": headers, "rows": body}
    except Exception as e:
        print(f"[CORR] read error: {e}")
        return {"headers": [], "rows": []}
# ---------------------------------------------------------------------------


# [ADD] ì—…ë¡œë“œëœ íŒŒì¼ë¡œ ì›Œí¬í”Œë¡œìš°ë¥¼ í•œ ë²ˆì— ì‹¤í–‰í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸
@app.post("/run_workflow/", response_class=HTMLResponse)
async def run_workflow(request: Request, sessionId: str = Form(None), filename: str = Form(None)):
    # íŒŒì¼ì´ ì—†ìœ¼ë©´ ì•ˆë‚´ë§Œ ë³´ì—¬ì¤Œ
    if sessionId not in session_files:
        generated_files = list_generated_files(sessionId)
        preview_images = [f for f in generated_files if f["ext"] in {".png", ".jpg", ".jpeg", ".gif", ".webp"}]
        return templates.TemplateResponse("index.html", {
            "request": request, "reply": "âš ï¸ ë¨¼ì € CSVë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.","current_session": None,
            "current_filename": None, "generated_files": generated_files, "preview_images": preview_images,
            "workflow": None, "steps": [], "head_columns": [], "head_rows": [],
            "describe_columns": [], "describe_rows": [],
            "corr": {"headers": [], "rows": []},
        })
    file_path = Path(session_files[sessionId])
    filename = file_path.name
    print(file_path, filename)

    if not file_path.exists():
        generated_files  = list_generated_files(sessionId)
        preview_images = [f for f in generated_files if f["ext"] in {".png",".jpg",".jpeg",".gif",".webp"}]
        return templates.TemplateResponse("index.html", {
            "request": request, "reply": "âš ï¸ ì—…ë¡œë“œëœ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", "current_session": sessionId,
            "current_filename": filename, "generated_files": generated_files, "preview_images": preview_images,
            "workflow": None, "steps": [], "head_columns": [], "head_rows": [],
            "describe_columns": [], "describe_rows": [],
            "corr": {"headers": [], "rows": []},
        })

    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    try:
        code, stdout, stderr = run_ts_workflow(file_path, sessionId, message="ë¶„ì„í•´ì¤˜")
        print(file_path, sessionId, filename)
        if code != 0:
            reply = f"âŒ ì˜¤ë¥˜: {stderr.strip() or 'unknown error'}"
            generated_files  = list_generated_files(sessionId)
            preview_images  = [f for f in generated_files if f["ext"] in {".png",".jpg",".jpeg",".gif",".webp"}]
            hc, hr, dc, dr = get_csv_preview(str(file_path))
            return templates.TemplateResponse("index.html", {
                "request": request, "reply": reply, "current_filename": filename, "current_session": sessionId,
                "generated_files": generated_files, "preview_images": preview_images, "workflow": None, "steps": [],
                "head_columns": hc, "head_rows": hr, "describe_columns": dc, "describe_rows": dr,
                "corr": {"headers": [], "rows": []},
            })

        # JSON íŒŒì‹± â†’ ì¹´ë“œ ë°ì´í„° êµ¬ì„±
        output_str = (stdout or "").strip()
        print("output_str", output_str)
        wf_raw, _ = extract_workflow_dict(output_str)

        # (ì„ íƒ) íŒŒì‹± ì‹¤íŒ¨ ì‹œ ìµœê·¼ ìƒì„± ì´ë¯¸ì§€ë¡œ ìµœì†Œ Visualization ì¹´ë“œë¼ë„ ë„ìš°ê¸°
        if not isinstance(wf_raw, dict):
            now = time.time()
            recent = []
            for p in OUTPUT_DIR.glob("*"):
                if p.is_file() and p.suffix.lower() in {".png",".jpg",".jpeg",".gif",".webp"}:
                    if now - p.stat().st_mtime <= 15:
                        recent.append(f"/outputs/{sessionId}/{p.name}")
            if recent:
                wf_raw = {"chartPaths": recent}

        # ë§¤í•‘/ìŠ¤í… êµ¬ì„±

        workflow_mapped = map_artifacts(wf_raw, sessionId) if isinstance(wf_raw, dict) else None

        # [NEW] Correlation CSV ë¡œë“œ â†’ í…œí”Œë¦¿ ì „ë‹¬ + steps ë°˜ì˜
        corr_csv_path = None
        root = OUTPUT_DIR / sessionId                      # â† src/outputs/<sessionId>
        if root.exists():
            # 1) íŒŒì¼ëª…ê³¼ ë”± ë§ëŠ” ê²ƒ ìš°ì„ 
            exact = root / f"{Path(filename).stem}.corr_matrix.csv"
            if exact.exists():
                corr_csv_path = str(exact)
            else:
                # 2) ì—†ìœ¼ë©´ í´ë” ì „ì²´ì—ì„œ íŒ¨í„´ ê²€ìƒ‰
                for q in root.rglob("*corr_matrix.csv"):
                    corr_csv_path = str(q)
                    break
        corr = _load_corr_csv(corr_csv_path)                  # [NEW]
        corr_has_table = bool(corr.get("headers"))            # [NEW]


        steps = build_steps(workflow_mapped, corr_has_table) if workflow_mapped else []  # [CHANGED]

        # íŒŒì¼/ë¯¸ë¦¬ë³´ê¸°/ê¸°ìˆ í†µê³„
        generated_files = list_generated_files(sessionId)
        preview_images = [f for f in generated_files if f["ext"] in {".png",".jpg",".jpeg",".gif",".webp"}]
        hc, hr, dc, dr = get_csv_preview(str(file_path))

        # í´ë°±: íŒŒì‹± ì‹¤íŒ¨í–ˆì§€ë§Œ ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ìµœì†Œ Visualization ì¹´ë“œë¼ë„ í‘œì‹œ
        if not workflow_mapped and preview_images:
            workflow_mapped = {"chartUrls": [img["url"] for img in preview_images]}
            steps = build_steps(workflow_mapped, corr_has_table)  # [CHANGED]


        print("[WF] keys:", list((workflow_mapped or {}).keys()))


        return templates.TemplateResponse("index.html", {
            "request": request, "current_filename": filename, "current_session":sessionId,
            "generated_files": generated_files, "preview_images": preview_images,
            "workflow": workflow_mapped, "steps": steps,
            "head_columns": hc, "head_rows": hr, "describe_columns": dc, "describe_rows": dr, "corr": corr,
        })

    except subprocess.TimeoutExpired:
        gf = list_generated_files(sessionId)
        pv = [f for f in gf if f["ext"] in {".png",".jpg",".jpeg",".gif",".webp"}]
        hc, hr, dc, dr = get_csv_preview(str(file_path))
        return templates.TemplateResponse("index.html", {
            "request": request, "reply": "âš ï¸ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼", "current_filename": filename,
            "generated_files": gf, "preview_images": pv,
            "workflow": None, "steps": [], "head_columns": hc, "head_rows": hr,
            "describe_columns": dc, "describe_rows": dr,
            "corr": {"headers": [], "rows": []},
        })
# ------------------------------
# ì±„íŒ…
# ------------------------------
@app.post("/chat/", response_class=HTMLResponse)
async def chat(request: Request, message: str = Form(...), sessionId: str = Form(None), filename: str = Form(None)):
    if sessionId not in session_files:
        reply = "âš ï¸ íŒŒì¼ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. CSVë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”."
        return templates.TemplateResponse("index.html", {"request": request, "reply": reply})

    file_path = Path(session_files[sessionId])
    filename = file_path.name
    chat_history = chat_histories.get(sessionId, [])
    chat_history.append({"role": "user", "content": message})

    try:
        cmd = [NPX, "ts-node", "src/main.ts", "--mode=chat", message, str(file_path), sessionId]
        proc = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            shell=False,
            text=True,
            encoding="utf-8",
            errors="replace",
            cwd=str(PROJECT_ROOT),
        )
        stdout, stderr = proc.communicate(timeout=600)
        output_str = sanitize_stdout(stdout)

        # âœ… í…ìŠ¤íŠ¸ + JSON ë¶„ë¦¬
        text_part, parsed_json = extract_json_and_text(output_str)

        # âœ… Markdown ìƒì„±
        md_output = ""
        if text_part:
            md_output += text_part + "\n\n"
        if parsed_json:
            md_output += format_tool_output(parsed_json, sessionId)

        # âœ… ì´ë¯¸ì§€/íŒŒì¼ ë§í¬ ì¶”ê°€
        generated_files = list_generated_files(sessionId)
        preview_images = [f for f in generated_files if f["ext"] in {".png", ".jpg", ".jpeg", ".gif", ".webp"}]
        other_files = [f for f in generated_files if f["ext"] in {".csv", ".json", ".txt", ".html", ".md"}]
        if preview_images or other_files:
            md_output += "\n\n### ğŸ“‚ ì‹œê°í™”/ê²°ê³¼ íŒŒì¼\n"
            for img in preview_images:
                rel = f"/outputs/{sessionId}/{img['name']}"
                md_output += f'<a href="{rel}" target="_blank"><img src="{rel}" alt="{img["name"]}" style="max-width:100%;height:auto;border-radius:8px;"/></a>\n'
            for f in other_files:
                rel = f"/outputs/{sessionId}/{f['name']}"
                md_output += f"- [{f['name']}]({rel})\n"

        chat_history.append({"role": "bot", "content": md_output})
        chat_histories[sessionId] = chat_history

        head_columns, head_rows, describe_columns, describe_rows = get_csv_preview(file_path)
        return templates.TemplateResponse("index.html", {
            "request": request,
            "chat_history": chat_history,
            "current_filename": filename,
            "current_session": sessionId,
            "head_columns": head_columns,
            "head_rows": head_rows,
            "describe_columns": describe_columns,
            "describe_rows": describe_rows,
            "generated_files": generated_files,
            "preview_images": preview_images,
            "corr": {"headers": [], "rows": []},
        })

    except subprocess.TimeoutExpired:
        reply = "âš ï¸ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼"
        chat_history.append({"role": "bot", "content": reply})
        chat_histories[sessionId] = chat_history
        return templates.TemplateResponse("index.html", {
            "request": request,
            "chat_history": chat_history,
            "reply": reply,
            "current_filename": filename,
        })
